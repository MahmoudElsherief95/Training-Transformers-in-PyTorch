{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e9d729",
   "metadata": {
    "id": "42e9d729"
   },
   "source": [
    "# Assignment 4: Training Transformers in PyTorch\n",
    "\n",
    "*Author:* Thomas Adler\n",
    "\n",
    "*Copyright statement:* This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors.\n",
    "\n",
    "In this assignment we will implement and train a small transformer model and compare it to the LSTM in the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7089693e",
   "metadata": {
    "id": "7089693e"
   },
   "source": [
    "## Exercise 1: Causal Self-Attention\n",
    "\n",
    "Write a class named `CausalSelfAttention` that derives from `nn.Module` and whose `__init__` method takes (apart from the trivial `self`) one argument `hidden_size`. Implement a method `forward` that takes an input sequence `x` of shape $(N, T, D)$ (where $N$ is batch size, $T$ is sequence length, $D$ is hidden size) and performs scaled dot-product self-attention, i.e.,\n",
    "$$\n",
    "Y = \\operatorname{softmax}\\left(\\frac{1}{\\sqrt{D}} Q K^\\top\\right) V,\n",
    "$$\n",
    "where $Q = X W_Q$ and $K = X W_K$ and $V = X W_V$ and $X \\in \\mathbb{R}^{T \\times D}$ and $W_Q, W_K, W_V \\in \\mathbb{R}^{D \\times D}$ and softmax is applied in a row-wise manner and neglecting bias units.\n",
    "It is called self-attention because $Q, K, V$ are all computed from the same input $X$, which hence attends to itself.\n",
    "\n",
    "To have the attention be *causal* we need to make sure that we do not allow peeks into the future. That is, the output at time $t$ must be a function of the input at times $1, \\dots, t$ but no further. The score matrix $E = \\frac{1}{\\sqrt{D}} Q K^\\top$ has a shape of $T \\times T$ and the entry $e_{ij}$ measures how strong the query at time $i$ attends to the key at time $j$. Therefore, positions where $j > i$ constitute peeks into the future and we have to set the corresponding attention values (i.e., the softmax-activated score) to zero. We can do that by setting the corresponding score to `float('-inf')`, which has the advantage that the normalization is adjusted automatically by the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1Q7CG-tCNySp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Q7CG-tCNySp",
    "outputId": "f5924808-2ae9-43b2-ec1d-afc70bce6695"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define the linear transformations for Q, K, and V\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Scale factor for the dot products\n",
    "        self.scale = 1 / math.sqrt(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_Q(x)\n",
    "        K = self.W_K(x)\n",
    "        V = self.W_V(x)\n",
    "\n",
    "        # Compute the dot product between queries and keys, and scale\n",
    "        E = torch.bmm(Q, K.transpose(1, 2)) * self.scale\n",
    "\n",
    "        # Mask out future positions (set them to -infinity)\n",
    "        mask = torch.tril(torch.ones(T, T)).unsqueeze(0).to(x.device)\n",
    "        E.masked_fill_(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get the attention weights\n",
    "        attn_weights = F.softmax(E, dim=-1)\n",
    "\n",
    "        # Multiply the attention weights with V\n",
    "        Y = torch.bmm(attn_weights, V)\n",
    "\n",
    "        return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "454c536d-0b4e-4eee-949b-2319e6661497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3778, -0.4509,  0.2248,  ..., -0.5050, -0.3929, -0.0457],\n",
       "         [ 0.4761,  0.5957,  0.6212,  ..., -0.7274, -0.6320, -0.6785],\n",
       "         [ 0.1303,  0.1449,  0.4196,  ..., -0.5367, -0.1259, -0.2969],\n",
       "         ...,\n",
       "         [ 0.1101, -0.1638,  0.1336,  ..., -0.4785, -0.0183, -0.1362],\n",
       "         [ 0.0949, -0.0545, -0.0398,  ...,  0.0605,  0.1543, -0.2989],\n",
       "         [-0.1204, -0.1010, -0.0717,  ..., -0.2489,  0.0578, -0.1628]]],\n",
       "       grad_fn=<BmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "hidden_size = 128\n",
    "model = CausalSelfAttention(hidden_size)\n",
    "x = torch.randn(1, 10, hidden_size)  \n",
    "output = model(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ee970",
   "metadata": {
    "id": "262ee970"
   },
   "source": [
    "## Exercise 2: Multi-Head Attention\n",
    "\n",
    "Write a class `MultiHeadCausalSelfAttention` that derives from `nn.Module` and extends the functionality of `CausalSelfAttention` from the previous exercise.\n",
    "The `__init__` method takes arguments `hidden_size, n_head, dropout`. `n_head` specifies the number of attention heads and `dropout` specifies the intensity for the dropout layers.\n",
    "The `forward` method should split the hidden dimension of the pre-activations (i.e., $Q, K, V$) in `n_head` equally sized parts and perform attention to these parts in parallel.\n",
    "Apply the first dropout layer direcly after the softmax.\n",
    "After the multiplication of the scores with the values, recombine the output of the distinct attention heads back into a single hidden dimension of size $D$, i.e., the resulting shape should be the shape of the input.\n",
    "Then perform an additional output projection again resulting in a hidden dimension of $D$.\n",
    "Finally, apply the second dropout layer after the output projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "mH2B1XrmQRbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH2B1XrmQRbc",
    "outputId": "51da1764-7594-4a7b-aa36-2f3aa76a59de"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(MultiHeadCausalSelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Ensure the hidden size is divisible by the number of heads\n",
    "        assert hidden_size % n_head == 0, \"Hidden size must be divisible by the number of heads.\"\n",
    "\n",
    "        self.head_dim = hidden_size // n_head\n",
    "\n",
    "        # Linear transformations for Q, K, and V\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Output linear transformation\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Dropout layers\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = 1 / math.sqrt(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        Q = self.W_Q(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_K(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_V(x).view(N, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        E = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Mask future positions\n",
    "        mask = torch.tril(torch.ones(T, T)).unsqueeze(0).unsqueeze(1).to(x.device)\n",
    "        E = E.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(E, dim=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        # Multiply attention weights with V\n",
    "        Y = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Recombine the heads\n",
    "        Y = Y.transpose(1, 2).contiguous().view(N, T, D)\n",
    "\n",
    "        # Output projection and dropout\n",
    "        Y = self.out_proj(Y)\n",
    "        Y = self.proj_dropout(Y)\n",
    "\n",
    "        return Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70891039-7b06-4b46-ab2b-1dc53579eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6956,  0.3825, -0.2999,  ...,  0.0163,  0.0000, -0.1558],\n",
       "         [-0.5959, -0.1812,  0.0973,  ..., -0.2410,  0.1236, -0.0000],\n",
       "         [-0.3223,  0.1045,  0.1389,  ..., -0.4915,  0.1925,  0.1823],\n",
       "         ...,\n",
       "         [-0.1890,  0.0134,  0.0197,  ..., -0.0000, -0.1011,  0.1121],\n",
       "         [-0.2518, -0.0104,  0.1993,  ..., -0.0000, -0.1611, -0.0221],\n",
       "         [-0.2400,  0.0327,  0.0367,  ..., -0.2354, -0.1295,  0.0158]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "hidden_size = 128\n",
    "n_head = 8\n",
    "dropout = 0.1\n",
    "model = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "x = torch.randn(1, 10, hidden_size)  \n",
    "output = model(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b329d",
   "metadata": {
    "id": "f81b329d"
   },
   "source": [
    "## Exercise 3: Multi-Layer Perceptron\n",
    "\n",
    "Write a class `MLP` that derives from `nn.Module` and whose `__init__` method takes two arguments: `hidden_size` and `dropout`.\n",
    "It should implement a 2-layer feedforward network with `hidden_size` inputs, `4*hidden_size` hiddens, and `hidden_size` outputs.\n",
    "It should apply the GELU activation function to the hiddens and dropout to the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "OQfce1FBReDT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQfce1FBReDT",
    "outputId": "45dbfd8f-fc7e-4b65-add5-0837743ee770"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # First layer of the MLP\n",
    "        self.fc1 = nn.Linear(hidden_size, 4 * hidden_size)\n",
    "\n",
    "        # Second layer of the MLP\n",
    "        self.fc2 = nn.Linear(4 * hidden_size, hidden_size)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first layer with GELU activation\n",
    "        x = F.gelu(self.fc1(x))\n",
    "\n",
    "        # Apply second layer and dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2a1ebe2-06ca-41e3-b91e-a7c9a17a9103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1872,  0.0944, -0.1092,  0.2600,  0.0093, -0.0484,  0.0811, -0.1332,\n",
       "          0.1413, -0.2939, -0.0675, -0.3201,  0.0483,  0.1950, -0.0668, -0.0880,\n",
       "          0.0545,  0.2328,  0.1142, -0.0090,  0.2061,  0.0000,  0.3525, -0.0056,\n",
       "         -0.2745,  0.1612, -0.0832, -0.4034,  0.1525,  0.2574, -0.0992, -0.0799,\n",
       "          0.1850, -0.0963, -0.3289, -0.0000,  0.2234, -0.2237,  0.0424, -0.3002,\n",
       "         -0.0000, -0.0000,  0.1193,  0.0962,  0.1384, -0.0549,  0.2893, -0.0873,\n",
       "         -0.0102,  0.4048,  0.3519, -0.1092, -0.0000,  0.0697,  0.0000,  0.3254,\n",
       "         -0.0241, -0.0390, -0.2991, -0.1073,  0.2787,  0.1819, -0.0318,  0.0377,\n",
       "         -0.2186, -0.1565, -0.2514, -0.0404,  0.0104, -0.4181,  0.2886,  0.0000,\n",
       "         -0.0277,  0.0000, -0.1043,  0.0426,  0.3145, -0.2582,  0.0105,  0.0687,\n",
       "          0.1905,  0.1326, -0.2883,  0.0371,  0.0871, -0.0859, -0.0649, -0.2406,\n",
       "          0.0000,  0.0986, -0.0915, -0.2288, -0.0565,  0.1765, -0.0000,  0.1401,\n",
       "         -0.1310, -0.2596, -0.0410, -0.0000,  0.1330,  0.4079,  0.1211,  0.2297,\n",
       "          0.1352, -0.0000, -0.1565, -0.0119, -0.1647,  0.2283,  0.1150,  0.0000,\n",
       "         -0.1933,  0.0442, -0.0677, -0.3391,  0.0577, -0.1987,  0.1887, -0.1472,\n",
       "         -0.4181,  0.0074, -0.3195,  0.5651, -0.0228, -0.3704,  0.0443,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "hidden_size = 128\n",
    "dropout = 0.1\n",
    "mlp = MLP(hidden_size, dropout)\n",
    "x = torch.randn(1, hidden_size)  # Example input\n",
    "output = mlp(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55645751",
   "metadata": {
    "id": "55645751"
   },
   "source": [
    "## Exercise 4: Block\n",
    "\n",
    "Write a class `Block` that derives from `nn.Module` and whose `__init__` method takes arguments `hidden_size, n_head, dropout`.\n",
    "It should apply `nn.LayerNorm`, `CausalMultiHeadSelfAttention`, `nn.LayerNorm`, `MLP` in that order and feature residual connections from the input to the output of `CausalMultiHeadSelfAttention` and from there to the output of `MLP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "B5b0mdmOTNMI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5b0mdmOTNMI",
    "outputId": "62f694bd-2df1-44c1-a43a-f48ba7757858"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, hidden_size, n_head, dropout):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Layer Normalization before Multi-Head Self-Attention\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Causal Multi-Head Self-Attention\n",
    "        self.attention = MultiHeadCausalSelfAttention(hidden_size, n_head, dropout)\n",
    "\n",
    "        # Layer Normalization before MLP\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Multi-Layer Perceptron\n",
    "        self.mlp = MLP(hidden_size, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply LayerNorm and Multi-Head Self-Attention with Residual Connection\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "\n",
    "        # Apply LayerNorm and MLP with Residual Connection\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d1fa559a-b73f-4f8b-adee-b2fd244a30b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5151, -1.4002,  0.4259,  ...,  1.2822, -0.1182, -0.5810],\n",
       "         [ 0.3529, -0.4463,  0.3599,  ..., -0.2832, -0.9787,  1.1137],\n",
       "         [ 0.5080, -0.1147,  0.9956,  ..., -1.8726, -0.8032,  0.4226],\n",
       "         ...,\n",
       "         [-1.3272, -1.9147, -0.0984,  ..., -0.9359,  0.0621,  1.3531],\n",
       "         [-0.7226, -0.6156,  0.3373,  ..., -1.2603, -2.0457,  1.1635],\n",
       "         [ 0.6738,  0.8762,  1.2101,  ...,  0.3614,  1.0999,  0.8395]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example \n",
    "hidden_size = 128\n",
    "n_head = 8\n",
    "dropout = 0.1\n",
    "block = Block(hidden_size, n_head, dropout)\n",
    "x = torch.randn(1, 10, hidden_size)  # Example input\n",
    "output = block(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b7022",
   "metadata": {
    "id": "138b7022"
   },
   "source": [
    "## Exercise 5: GPT\n",
    "\n",
    "Write a class `GPT` that derives from `nn.Module` and whose `__init__` method takes arguments `vocab_size, context_size, hidden_size, n_layer, n_head, dropout`.\n",
    "The `forward` method should take two arguments `x, y` representing sequences of input and target tokens, respectively, both of which have type `torch.long` and shape ($N$, $T$), and returns logits and loss as a tuple.\n",
    "The `GPT` module should feature two `nn.Embedding` layers, one for token embeddings and one for positional embedding, i.e., it should embed the position of the corresponding token within the input sequence.\n",
    "The positional embedding is necessary for the Transformer to determine the order of its inputs.\n",
    "Add the two embeddings and apply a dropout layer.\n",
    "Next, apply `n_layers` layers of `Block`s followed by a `nn.LayerNorm` and a `nn.Linear` (without bias) mapping to an output dimension of `vocab_size`.\n",
    "Finally, apply the cross-entropy loss function to the logits.\n",
    "To save some parameters, apply weight tying between the token embedding layer and the output layer, i.e., they should use the same weights.\n",
    "Initialize all weights using a normal distribution with a mean of zero and a standard deviation of 0.02 (except for the output layers of the `MLP`s use $0.02/\\sqrt{2 * \\mathtt{n\\_layer}}$) and all biases to zero.\n",
    "Use the argument `dropout` as intensity for all dropout layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6907hxEqTxvZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6907hxEqTxvZ",
    "outputId": "9a2f4635-171c-4e50-bd70-1c0282cef769"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(context_size, hidden_size)\n",
    "\n",
    "        # Apply weight tying between token embeddings and the output layer\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embeddings.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # GPT Blocks\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "\n",
    "        # Final Layer Normalization\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.init_weights(n_layer)\n",
    "\n",
    "    def init_weights(self, n_layer):\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                if 'mlp' in name and 'weight' in name:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "            else:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        N, T = x.size()\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "        position_embeddings = self.position_embeddings(torch.arange(T, device=x.device)).unsqueeze(0).repeat(N, 1, 1)\n",
    "        x = self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "        # Apply GPT Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Output linear layer to get logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc12d6",
   "metadata": {
    "id": "fcdc12d6"
   },
   "source": [
    "## Exercise 6: Optimizer\n",
    "\n",
    "Add a method `configure_optimizers` to the class `GPT` that takes arguments `weight_decay, learning_rate, betas`.\n",
    "Divide the model parameters into two groups.\n",
    "The first group consists of all parameters with at least 2 dimensions, e.g., weight/embedding matrices and uses a decay of `weight_decay`.\n",
    "The second group consists of all other parameters, e.g., biases and layer norms, and does not use weight decay.\n",
    "Construct and return a `torch.optim.AdamW` optimizer with `learning_rate` and `betas` that operates on these two parameter groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "G6jSDellUuyK",
   "metadata": {
    "id": "G6jSDellUuyK"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Import torch.optim\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(context_size, hidden_size)\n",
    "\n",
    "        # Apply weight tying between token embeddings and the output layer\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embeddings.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # GPT Blocks\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "\n",
    "        # Final Layer Normalization\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.init_weights(n_layer)\n",
    "\n",
    "    def init_weights(self, n_layer):\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                if 'mlp' in name and 'weight' in name:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "            else:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        N, T = x.size()\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "        position_embeddings = self.position_embeddings(torch.arange(T, device=x.device)).unsqueeze(0).repeat(N, 1, 1)\n",
    "        x = self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "        # Apply GPT Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Output linear layer to get logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "        # Group parameters: weight matrices (2 or more dimensions) and others\n",
    "        decay_params = [p for name, p in self.named_parameters() if p.dim() > 1]\n",
    "        no_decay_params = [p for name, p in self.named_parameters() if p.dim() == 1]\n",
    "\n",
    "        # Create two parameter groups\n",
    "        param_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "        ]\n",
    "\n",
    "        # Construct AdamW optimizer with the specified parameters and hyperparameters\n",
    "        optimizer = optim.AdamW(param_groups, lr=learning_rate, betas=betas)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1b472",
   "metadata": {
    "id": "96a1b472"
   },
   "source": [
    "## Exercise 7: Training\n",
    "\n",
    "In the code cell below you find some globals, helper functions, and boilerplate code. Extend the given code by a training loop that\n",
    "* stops after `max_iters` iterations\n",
    "* applies the learning rate schedule implemented in `get_lr`\n",
    "* applies gradient clipping at `grad_clip` using `torch.nn.utils.clip_grad_norm_`\n",
    "* accumulates gradients for `gradient_accumulation_steps` batches before each weight update\n",
    "* logs the training loss and learning rate every `log_interval` iterations\n",
    "* evaluates (and potentially checkpoints) the model using `estimate_loss` every `eval_iters` iterations.\n",
    "\n",
    "The provided hyperparameter values should be a good guess for training a tiny model on CPU but feel free to experiment with them as you please. In particular, if you have a GPU available, you can try to scale things up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "641a76a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "641a76a3",
    "outputId": "55c8f904-1c71-4cc7-a5ee-27458b3c69ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.0922, Learning Rate: 0.000000\n",
      "Iteration 10, Loss: 0.0922, Learning Rate: 0.000100\n",
      "Iteration 20, Loss: 0.0922, Learning Rate: 0.000200\n",
      "Iteration 30, Loss: 0.0922, Learning Rate: 0.000300\n",
      "Iteration 40, Loss: 0.0922, Learning Rate: 0.000400\n",
      "Iteration 50, Loss: 0.0922, Learning Rate: 0.000500\n",
      "Iteration 60, Loss: 0.0922, Learning Rate: 0.000600\n",
      "Iteration 70, Loss: 0.0922, Learning Rate: 0.000700\n",
      "Iteration 80, Loss: 0.0922, Learning Rate: 0.000800\n",
      "Iteration 90, Loss: 0.0922, Learning Rate: 0.000900\n",
      "Iteration 100, Loss: 0.0922, Learning Rate: 0.001000\n",
      "3.2724000536029276 iterations/sec\n",
      "Iteration 110, Loss: 0.0922, Learning Rate: 0.001000\n",
      "Iteration 120, Loss: 0.0922, Learning Rate: 0.001000\n",
      "Iteration 130, Loss: 0.0922, Learning Rate: 0.000999\n",
      "Iteration 140, Loss: 0.0922, Learning Rate: 0.000999\n",
      "Iteration 150, Loss: 0.0922, Learning Rate: 0.000998\n",
      "Iteration 160, Loss: 0.0921, Learning Rate: 0.000998\n",
      "Iteration 170, Loss: 0.0921, Learning Rate: 0.000997\n",
      "Iteration 180, Loss: 0.0921, Learning Rate: 0.000996\n",
      "Iteration 190, Loss: 0.0921, Learning Rate: 0.000995\n",
      "Iteration 200, Loss: 0.0920, Learning Rate: 0.000994\n",
      "3.3538281305254642 iterations/sec\n",
      "Iteration 210, Loss: 0.0921, Learning Rate: 0.000993\n",
      "Iteration 220, Loss: 0.0921, Learning Rate: 0.000991\n",
      "Iteration 230, Loss: 0.0921, Learning Rate: 0.000990\n",
      "Iteration 240, Loss: 0.0920, Learning Rate: 0.000988\n",
      "Iteration 250, Loss: 0.0920, Learning Rate: 0.000986\n",
      "Validation Loss at Iteration 250: 3.6795\n",
      "Iteration 260, Loss: 0.0920, Learning Rate: 0.000984\n",
      "Iteration 270, Loss: 0.0920, Learning Rate: 0.000982\n",
      "Iteration 280, Loss: 0.0919, Learning Rate: 0.000980\n",
      "Iteration 290, Loss: 0.0919, Learning Rate: 0.000978\n",
      "Iteration 300, Loss: 0.0919, Learning Rate: 0.000976\n",
      "3.0529219582601774 iterations/sec\n",
      "Iteration 310, Loss: 0.0919, Learning Rate: 0.000973\n",
      "Iteration 320, Loss: 0.0918, Learning Rate: 0.000971\n",
      "Iteration 330, Loss: 0.0918, Learning Rate: 0.000968\n",
      "Iteration 340, Loss: 0.0918, Learning Rate: 0.000965\n",
      "Iteration 350, Loss: 0.0918, Learning Rate: 0.000962\n",
      "Iteration 360, Loss: 0.0917, Learning Rate: 0.000959\n",
      "Iteration 370, Loss: 0.0917, Learning Rate: 0.000956\n",
      "Iteration 380, Loss: 0.0917, Learning Rate: 0.000953\n",
      "Iteration 390, Loss: 0.0917, Learning Rate: 0.000949\n",
      "Iteration 400, Loss: 0.0916, Learning Rate: 0.000946\n",
      "3.384410832596063 iterations/sec\n",
      "Iteration 410, Loss: 0.0916, Learning Rate: 0.000942\n",
      "Iteration 420, Loss: 0.0916, Learning Rate: 0.000938\n",
      "Iteration 430, Loss: 0.0916, Learning Rate: 0.000935\n",
      "Iteration 440, Loss: 0.0915, Learning Rate: 0.000931\n",
      "Iteration 450, Loss: 0.0915, Learning Rate: 0.000927\n",
      "Iteration 460, Loss: 0.0915, Learning Rate: 0.000923\n",
      "Iteration 470, Loss: 0.0915, Learning Rate: 0.000918\n",
      "Iteration 480, Loss: 0.0914, Learning Rate: 0.000914\n",
      "Iteration 490, Loss: 0.0914, Learning Rate: 0.000910\n",
      "Iteration 500, Loss: 0.0913, Learning Rate: 0.000905\n",
      "Validation Loss at Iteration 500: 3.6540\n",
      "2.9828742374966994 iterations/sec\n",
      "Iteration 510, Loss: 0.0914, Learning Rate: 0.000900\n",
      "Iteration 520, Loss: 0.0912, Learning Rate: 0.000896\n",
      "Iteration 530, Loss: 0.0912, Learning Rate: 0.000891\n",
      "Iteration 540, Loss: 0.0912, Learning Rate: 0.000886\n",
      "Iteration 550, Loss: 0.0912, Learning Rate: 0.000881\n",
      "Iteration 560, Loss: 0.0911, Learning Rate: 0.000876\n",
      "Iteration 570, Loss: 0.0911, Learning Rate: 0.000871\n",
      "Iteration 580, Loss: 0.0911, Learning Rate: 0.000866\n",
      "Iteration 590, Loss: 0.0911, Learning Rate: 0.000860\n",
      "Iteration 600, Loss: 0.0909, Learning Rate: 0.000855\n",
      "3.0699825657537625 iterations/sec\n",
      "Iteration 610, Loss: 0.0909, Learning Rate: 0.000849\n",
      "Iteration 620, Loss: 0.0909, Learning Rate: 0.000844\n",
      "Iteration 630, Loss: 0.0909, Learning Rate: 0.000838\n",
      "Iteration 640, Loss: 0.0908, Learning Rate: 0.000832\n",
      "Iteration 650, Loss: 0.0908, Learning Rate: 0.000826\n",
      "Iteration 660, Loss: 0.0907, Learning Rate: 0.000820\n",
      "Iteration 670, Loss: 0.0907, Learning Rate: 0.000815\n",
      "Iteration 680, Loss: 0.0906, Learning Rate: 0.000808\n",
      "Iteration 690, Loss: 0.0906, Learning Rate: 0.000802\n",
      "Iteration 700, Loss: 0.0906, Learning Rate: 0.000796\n",
      "3.04354753408557 iterations/sec\n",
      "Iteration 710, Loss: 0.0906, Learning Rate: 0.000790\n",
      "Iteration 720, Loss: 0.0905, Learning Rate: 0.000784\n",
      "Iteration 730, Loss: 0.0904, Learning Rate: 0.000777\n",
      "Iteration 740, Loss: 0.0904, Learning Rate: 0.000771\n",
      "Iteration 750, Loss: 0.0904, Learning Rate: 0.000764\n",
      "Validation Loss at Iteration 750: 3.6164\n",
      "Iteration 760, Loss: 0.0903, Learning Rate: 0.000758\n",
      "Iteration 770, Loss: 0.0903, Learning Rate: 0.000751\n",
      "Iteration 780, Loss: 0.0903, Learning Rate: 0.000744\n",
      "Iteration 790, Loss: 0.0903, Learning Rate: 0.000738\n",
      "Iteration 800, Loss: 0.0901, Learning Rate: 0.000731\n",
      "2.9852120438725183 iterations/sec\n",
      "Iteration 810, Loss: 0.0901, Learning Rate: 0.000724\n",
      "Iteration 820, Loss: 0.0901, Learning Rate: 0.000717\n",
      "Iteration 830, Loss: 0.0901, Learning Rate: 0.000710\n",
      "Iteration 840, Loss: 0.0898, Learning Rate: 0.000703\n",
      "Iteration 850, Loss: 0.0899, Learning Rate: 0.000696\n",
      "Iteration 860, Loss: 0.0900, Learning Rate: 0.000689\n",
      "Iteration 870, Loss: 0.0900, Learning Rate: 0.000682\n",
      "Iteration 880, Loss: 0.0897, Learning Rate: 0.000675\n",
      "Iteration 890, Loss: 0.0899, Learning Rate: 0.000668\n",
      "Iteration 900, Loss: 0.0898, Learning Rate: 0.000660\n",
      "3.2294804663817165 iterations/sec\n",
      "Iteration 910, Loss: 0.0897, Learning Rate: 0.000653\n",
      "Iteration 920, Loss: 0.0896, Learning Rate: 0.000646\n",
      "Iteration 930, Loss: 0.0896, Learning Rate: 0.000639\n",
      "Iteration 940, Loss: 0.0896, Learning Rate: 0.000631\n",
      "Iteration 950, Loss: 0.0894, Learning Rate: 0.000624\n",
      "Iteration 960, Loss: 0.0893, Learning Rate: 0.000617\n",
      "Iteration 970, Loss: 0.0894, Learning Rate: 0.000609\n",
      "Iteration 980, Loss: 0.0895, Learning Rate: 0.000602\n",
      "Iteration 990, Loss: 0.0893, Learning Rate: 0.000595\n",
      "Iteration 1000, Loss: 0.0891, Learning Rate: 0.000587\n",
      "Validation Loss at Iteration 1000: 3.5652\n",
      "2.9679508808454163 iterations/sec\n",
      "Iteration 1010, Loss: 0.0892, Learning Rate: 0.000580\n",
      "Iteration 1020, Loss: 0.0893, Learning Rate: 0.000572\n",
      "Iteration 1030, Loss: 0.0892, Learning Rate: 0.000565\n",
      "Iteration 1040, Loss: 0.0891, Learning Rate: 0.000557\n",
      "Iteration 1050, Loss: 0.0890, Learning Rate: 0.000550\n",
      "Iteration 1060, Loss: 0.0891, Learning Rate: 0.000543\n",
      "Iteration 1070, Loss: 0.0891, Learning Rate: 0.000535\n",
      "Iteration 1080, Loss: 0.0889, Learning Rate: 0.000528\n",
      "Iteration 1090, Loss: 0.0891, Learning Rate: 0.000520\n",
      "Iteration 1100, Loss: 0.0889, Learning Rate: 0.000513\n",
      "3.2938321228435257 iterations/sec\n",
      "Iteration 1110, Loss: 0.0890, Learning Rate: 0.000505\n",
      "Iteration 1120, Loss: 0.0888, Learning Rate: 0.000498\n",
      "Iteration 1130, Loss: 0.0886, Learning Rate: 0.000491\n",
      "Iteration 1140, Loss: 0.0886, Learning Rate: 0.000483\n",
      "Iteration 1150, Loss: 0.0889, Learning Rate: 0.000476\n",
      "Iteration 1160, Loss: 0.0886, Learning Rate: 0.000469\n",
      "Iteration 1170, Loss: 0.0886, Learning Rate: 0.000461\n",
      "Iteration 1180, Loss: 0.0885, Learning Rate: 0.000454\n",
      "Iteration 1190, Loss: 0.0888, Learning Rate: 0.000447\n",
      "Iteration 1200, Loss: 0.0885, Learning Rate: 0.000440\n",
      "3.2271893757977077 iterations/sec\n",
      "Iteration 1210, Loss: 0.0883, Learning Rate: 0.000432\n",
      "Iteration 1220, Loss: 0.0884, Learning Rate: 0.000425\n",
      "Iteration 1230, Loss: 0.0885, Learning Rate: 0.000418\n",
      "Iteration 1240, Loss: 0.0884, Learning Rate: 0.000411\n",
      "Iteration 1250, Loss: 0.0885, Learning Rate: 0.000404\n",
      "Validation Loss at Iteration 1250: 3.5300\n",
      "Iteration 1260, Loss: 0.0882, Learning Rate: 0.000397\n",
      "Iteration 1270, Loss: 0.0884, Learning Rate: 0.000390\n",
      "Iteration 1280, Loss: 0.0882, Learning Rate: 0.000383\n",
      "Iteration 1290, Loss: 0.0883, Learning Rate: 0.000376\n",
      "Iteration 1300, Loss: 0.0881, Learning Rate: 0.000369\n",
      "2.94234155306268 iterations/sec\n",
      "Iteration 1310, Loss: 0.0882, Learning Rate: 0.000362\n",
      "Iteration 1320, Loss: 0.0881, Learning Rate: 0.000356\n",
      "Iteration 1330, Loss: 0.0880, Learning Rate: 0.000349\n",
      "Iteration 1340, Loss: 0.0881, Learning Rate: 0.000342\n",
      "Iteration 1350, Loss: 0.0880, Learning Rate: 0.000336\n",
      "Iteration 1360, Loss: 0.0879, Learning Rate: 0.000329\n",
      "Iteration 1370, Loss: 0.0879, Learning Rate: 0.000323\n",
      "Iteration 1380, Loss: 0.0880, Learning Rate: 0.000316\n",
      "Iteration 1390, Loss: 0.0879, Learning Rate: 0.000310\n",
      "Iteration 1400, Loss: 0.0878, Learning Rate: 0.000304\n",
      "3.266094660292684 iterations/sec\n",
      "Iteration 1410, Loss: 0.0878, Learning Rate: 0.000298\n",
      "Iteration 1420, Loss: 0.0881, Learning Rate: 0.000292\n",
      "Iteration 1430, Loss: 0.0878, Learning Rate: 0.000285\n",
      "Iteration 1440, Loss: 0.0879, Learning Rate: 0.000280\n",
      "Iteration 1450, Loss: 0.0877, Learning Rate: 0.000274\n",
      "Iteration 1460, Loss: 0.0879, Learning Rate: 0.000268\n",
      "Iteration 1470, Loss: 0.0876, Learning Rate: 0.000262\n",
      "Iteration 1480, Loss: 0.0876, Learning Rate: 0.000256\n",
      "Iteration 1490, Loss: 0.0876, Learning Rate: 0.000251\n",
      "Iteration 1500, Loss: 0.0879, Learning Rate: 0.000245\n",
      "Validation Loss at Iteration 1500: 3.4981\n",
      "2.936212371418673 iterations/sec\n",
      "Iteration 1510, Loss: 0.0876, Learning Rate: 0.000240\n",
      "Iteration 1520, Loss: 0.0878, Learning Rate: 0.000234\n",
      "Iteration 1530, Loss: 0.0876, Learning Rate: 0.000229\n",
      "Iteration 1540, Loss: 0.0874, Learning Rate: 0.000224\n",
      "Iteration 1550, Loss: 0.0875, Learning Rate: 0.000219\n",
      "Iteration 1560, Loss: 0.0876, Learning Rate: 0.000214\n",
      "Iteration 1570, Loss: 0.0874, Learning Rate: 0.000209\n",
      "Iteration 1580, Loss: 0.0873, Learning Rate: 0.000204\n",
      "Iteration 1590, Loss: 0.0876, Learning Rate: 0.000200\n",
      "Iteration 1600, Loss: 0.0874, Learning Rate: 0.000195\n",
      "3.210238243107774 iterations/sec\n",
      "Iteration 1610, Loss: 0.0873, Learning Rate: 0.000190\n",
      "Iteration 1620, Loss: 0.0875, Learning Rate: 0.000186\n",
      "Iteration 1630, Loss: 0.0874, Learning Rate: 0.000182\n",
      "Iteration 1640, Loss: 0.0871, Learning Rate: 0.000177\n",
      "Iteration 1650, Loss: 0.0874, Learning Rate: 0.000173\n",
      "Iteration 1660, Loss: 0.0874, Learning Rate: 0.000169\n",
      "Iteration 1670, Loss: 0.0872, Learning Rate: 0.000165\n",
      "Iteration 1680, Loss: 0.0871, Learning Rate: 0.000162\n",
      "Iteration 1690, Loss: 0.0872, Learning Rate: 0.000158\n",
      "Iteration 1700, Loss: 0.0873, Learning Rate: 0.000154\n",
      "3.274463619044318 iterations/sec\n",
      "Iteration 1710, Loss: 0.0872, Learning Rate: 0.000151\n",
      "Iteration 1720, Loss: 0.0873, Learning Rate: 0.000147\n",
      "Iteration 1730, Loss: 0.0872, Learning Rate: 0.000144\n",
      "Iteration 1740, Loss: 0.0871, Learning Rate: 0.000141\n",
      "Iteration 1750, Loss: 0.0872, Learning Rate: 0.000138\n",
      "Validation Loss at Iteration 1750: 3.4848\n",
      "Iteration 1760, Loss: 0.0871, Learning Rate: 0.000135\n",
      "Iteration 1770, Loss: 0.0871, Learning Rate: 0.000132\n",
      "Iteration 1780, Loss: 0.0872, Learning Rate: 0.000129\n",
      "Iteration 1790, Loss: 0.0872, Learning Rate: 0.000127\n",
      "Iteration 1800, Loss: 0.0871, Learning Rate: 0.000124\n",
      "2.9825625181641175 iterations/sec\n",
      "Iteration 1810, Loss: 0.0872, Learning Rate: 0.000122\n",
      "Iteration 1820, Loss: 0.0869, Learning Rate: 0.000120\n",
      "Iteration 1830, Loss: 0.0869, Learning Rate: 0.000118\n",
      "Iteration 1840, Loss: 0.0870, Learning Rate: 0.000116\n",
      "Iteration 1850, Loss: 0.0870, Learning Rate: 0.000114\n",
      "Iteration 1860, Loss: 0.0873, Learning Rate: 0.000112\n",
      "Iteration 1870, Loss: 0.0870, Learning Rate: 0.000110\n",
      "Iteration 1880, Loss: 0.0871, Learning Rate: 0.000109\n",
      "Iteration 1890, Loss: 0.0870, Learning Rate: 0.000107\n",
      "Iteration 1900, Loss: 0.0870, Learning Rate: 0.000106\n",
      "3.2818656878871324 iterations/sec\n",
      "Iteration 1910, Loss: 0.0869, Learning Rate: 0.000105\n",
      "Iteration 1920, Loss: 0.0872, Learning Rate: 0.000104\n",
      "Iteration 1930, Loss: 0.0873, Learning Rate: 0.000103\n",
      "Iteration 1940, Loss: 0.0870, Learning Rate: 0.000102\n",
      "Iteration 1950, Loss: 0.0871, Learning Rate: 0.000102\n",
      "Iteration 1960, Loss: 0.0870, Learning Rate: 0.000101\n",
      "Iteration 1970, Loss: 0.0869, Learning Rate: 0.000101\n",
      "Iteration 1980, Loss: 0.0870, Learning Rate: 0.000100\n",
      "Iteration 1990, Loss: 0.0866, Learning Rate: 0.000100\n"
     ]
    }
   ],
   "source": [
    "eval_interval = 250 # validate model every .. iterations\n",
    "log_interval = 10 # log training loss every .. iterations\n",
    "eval_iters = 20 # number of batches for loss estimation\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger training batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_size = 64 # sequence length\n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz0123456789 .!?' # vocabulary\n",
    "vocab_size = len(vocab) # 40\n",
    "n_layer = 16 # number of layers\n",
    "n_head = 16 # number of attention heads\n",
    "hidden_size = 128 # layer size\n",
    "dropout = 0.1 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "learning_rate = 1e-3 # max learning rate\n",
    "max_iters = 2000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9 # for AdamW\n",
    "beta2 = 0.99 # for AdamW\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable with 0.0\n",
    "warmup_iters = 100 # how many steps to warm up for\n",
    "min_lr = 1e-4 # minimum learning rate, usually ~= learning_rate/10\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > max_iters, return min learning rate\n",
    "    if it > max_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (max_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def load_data(split):\n",
    "    import re\n",
    "\n",
    "    with open(f'trump_{split}.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text = text.lower() # convert to lower case\n",
    "    text = re.sub('[^a-z0-9 .!?]', ' ', text) # replace all unknown chars with ' '\n",
    "    text = re.sub(' +', ' ', text) # reduce multiple blanks to one\n",
    "    text = [vocab.index(t) for t in text]\n",
    "    text = torch.tensor(text, dtype=torch.long)\n",
    "    return text\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+context_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# data, model, optimizer, etc.\n",
    "train_data = load_data('train')\n",
    "val_data = load_data('val')\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head ,dropout)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2))\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "# Training Loop\n",
    "for iter_num in range(max_iters):\n",
    "    # Adjust learning rate according to the schedule\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Initialize gradients\n",
    "    if iter_num % gradient_accumulation_steps == 0:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Get a batch of data\n",
    "    X, Y = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = model(X, Y)\n",
    "\n",
    "    # Normalize loss to account for gradient accumulation\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization step after accumulating gradients\n",
    "    if (iter_num + 1) % gradient_accumulation_steps == 0:\n",
    "        # Apply gradient clipping\n",
    "        if grad_clip > 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    # Log training progress\n",
    "    if iter_num % log_interval == 0:\n",
    "        print(f\"Iteration {iter_num}, Loss: {loss.item():.4f}, Learning Rate: {lr:.6f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    if iter_num % eval_interval == 0 and iter_num > 0:\n",
    "        val_loss = estimate_loss()['val']\n",
    "        print(f\"Validation Loss at Iteration {iter_num}: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Save the model checkpoint here if desired\n",
    "\n",
    "    # Time keeping\n",
    "    if iter_num % 100 == 0 and iter_num > 0:\n",
    "        t1 = time.time()\n",
    "        print(f\"{100 / (t1 - t0)} iterations/sec\")\n",
    "        t0 = t1\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f0e3c",
   "metadata": {
    "id": "7b4f0e3c"
   },
   "source": [
    "## Exercise 8: Inference\n",
    "\n",
    "Add a method `generate` to the class `GPT` that takes arguments `x, max_new_tokens, temperature=1.0`.\n",
    "The method should take a batch of token sequences `x`, which it should extend by `max_new_tokens` new tokens generated by the model.\n",
    "Once you have computed the logits for the next token, divide them by `temperature` before applying the softmax.\n",
    "After applying the softmax, sample the next token from the resulting categorical distribution.\n",
    "Try out different values for `temperature` and compare the results to those from the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e921b",
   "metadata": {
    "id": "595e921b"
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c504f763",
   "metadata": {
    "id": "c504f763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helhf.v0z5jvt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # Import torch.optim\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, hidden_size, n_layer, n_head, dropout):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(context_size, hidden_size)\n",
    "\n",
    "        # Apply weight tying between token embeddings and the output layer\n",
    "        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embeddings.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # GPT Blocks\n",
    "        self.blocks = nn.ModuleList([Block(hidden_size, n_head, dropout) for _ in range(n_layer)])\n",
    "\n",
    "        # Final Layer Normalization\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.init_weights(n_layer)\n",
    "\n",
    "    def init_weights(self, n_layer):\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                if 'mlp' in name and 'weight' in name:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    nn.init.normal_(p, mean=0.0, std=0.02)\n",
    "            else:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        N, T = x.size()\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "        position_embeddings = self.position_embeddings(torch.arange(T, device=x.device)).unsqueeze(0).repeat(N, 1, 1)\n",
    "        x = self.dropout(token_embeddings + position_embeddings)\n",
    "\n",
    "        # Apply GPT Blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Output linear layer to get logits\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(logits.view(-1, self.vocab_size), y.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas):\n",
    "        # Group parameters: weight matrices (2 or more dimensions) and others\n",
    "        decay_params = [p for name, p in self.named_parameters() if p.dim() > 1]\n",
    "        no_decay_params = [p for name, p in self.named_parameters() if p.dim() == 1]\n",
    "\n",
    "        # Create two parameter groups\n",
    "        param_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": no_decay_params, \"weight_decay\": 0.0}\n",
    "        ]\n",
    "\n",
    "        # Construct AdamW optimizer with the specified parameters and hyperparameters\n",
    "        optimizer = optim.AdamW(param_groups, lr=learning_rate, betas=betas)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "    def generate(self, x, max_new_tokens, temperature=1.0):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(x, x)  # Obtain logits for the next token\n",
    "            # Get logits of the last token in the sequence\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from the categorical distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # Append the sampled token to the sequence\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Create an instance of the GPT model\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head, dropout)\n",
    "\n",
    "# Prepare input data\n",
    "x_start = \"Hel\"\n",
    "x_start = x_start.lower()\n",
    "x_indices = [vocab.index(char) for char in x_start]\n",
    "x = torch.tensor([x_indices], dtype=torch.long)\n",
    "\n",
    "# Set generation parameters\n",
    "max_new_tokens = 10\n",
    "temperature = 0.7\n",
    "\n",
    "# Generate the sequence\n",
    "generated_sequence = model.generate(x, max_new_tokens, temperature)\n",
    "\n",
    "# Convert generated indices back to characters\n",
    "generated_text = ''.join([vocab[idx] for idx in generated_sequence[0].tolist()])\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63c95bba-6c4c-4449-82a8-319a7e6148e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.01: helqvt011cu x\n",
      "Temperature 0.1: helzpzqooi.pw\n",
      "Temperature 0.5: hel3q7dty9vep\n",
      "Temperature 0.7: heliq30ply9.4\n",
      "Temperature 1.0: helntd?8vij29\n",
      "Temperature 1.5: hel7n 3j65of9\n",
      "Temperature 2.0: helt7875kcq9!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPT(vocab_size, context_size, hidden_size, n_layer, n_head, dropout)\n",
    "\n",
    "# Prepare input data\n",
    "x_start = \"Hel\"\n",
    "x_start = x_start.lower()\n",
    "x_indices = [vocab.index(char) for char in x_start]\n",
    "x = torch.tensor([x_indices], dtype=torch.long)\n",
    "\n",
    "# Set up a range of temperature values to try\n",
    "temperatures = [ 0.01,0.1,0.5, 0.7, 1.0, 1.5, 2.0]\n",
    "max_new_tokens = 10\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Generate the sequence with the current temperature\n",
    "    generated_sequence = model.generate(x, max_new_tokens, temp)\n",
    "\n",
    "    # Convert generated indices back to characters\n",
    "    generated_text = ''.join([vocab[idx] for idx in generated_sequence[0].tolist()])\n",
    "\n",
    "    print(f\"Temperature {temp}: {generated_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ef151-151b-4c11-8adf-6830ff2f3359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
